= AI
:toc: manual

== Generative AI(生成式人工智能)

=== Definition

* Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data.

=== How Generative AI Works

Generative AI is a type of artificial intelligence technology that creates new content based on what it has learned from existing content.

* The process of *learning from existing content* is called training and results in the creation of a statistical model.
* When given a prompt, Generative AI uses this *statistical model* to predict what an expected response might be and generates new content.

=== Generative AI Model Types

[cols="2,5a"]
.*Generative language models*
|===
|Item |Generative `language` models 

|Definition
|Generative language models learn about patterns in language through training data.

Then, give some text, they predict *what comes next*.

Generative language models are pattern-matching systems.

|Input
|Text

|Output
|

* *Text* - Translation, Summarization, Question Answering, Grammar Correction
* *Image* - Image Generation, Video Generation
* *Audio* - Text to Speech
* *Decisions* - Play Games

|===

[cols="2,5a"]
.*Generative image models*
|===
|Item |Generative `image` models

|Definition
|Generative image models produce new images using techniques like diffusion.

Then give a prompt or related imagery, they *transform random noise into image or generate images from prompts*.

|Input
|Image

|Output
|

* *Text* - Image Captioning, Visual Question Answering, Image Search 
* *Image* - Super Resolution, Image Completion
* *Video* - Animation
|===

=== Generative Applications

|===
|TYPE | DESC

|Text
|Marketing(content), Sales(mail), Support(chat/email), General writing, Note taking, etc

|Code
|Code generation, Code Documentation, Text to SQL, Web app builders

|Image
|Image generation, Consumer/Social, Media/Advertising, Design

|Speech
|Voice Synthesis

|Vedio
|Vedio editing/generation

|3D
|3D Models/scenes

|Other
|Gaming, RPA, Music, Audio, Biology & chemistry

|===

== Large Language Models

== Encoder-Decoder Architecture

To begin with the encoder-decoder architecture, is a sequence to sequence architecture, this means  that it takes, for example a sequence of words as input, like the sentence in English, “the cat ate the mouse”, and it’s outputs the translation in Chinese”大黄猫吃了老鼠”. The encoder-decoder architecture is a machine that consumes sequences and spits out sequences.Another input example is the sequence of words forming the prompt sent to a large language model, then the output is the response of the large language model of this prompt.

Now we know what an encoder-decoder architecture does, but how does it do this? Typically the encoder-decoder architecture has two stages, first an encoder stage, that produces a vector representation of the input sentence. Then this encoder stage is followed by a decoder stage that creates the sentence output. Both encoder and decoder can be implemented with different internal architectures. The internal mechanism can be a recurrent neural network, or a more complex transformer block, as in the case of the super powerful language models we see nowadays. 

A recurrent neural network encoder takes each token in the input sequence one at a time and produces a state representing this token as well as all the previously ingested tokens. Then the state is used in the next encoding step as input along with the next token to produce the next state, once you are done ingesting all the input tokens into the RNN, you output a vector that essentially represents the full input sentence. That’s it for encoder. What about the decoder parts? The decoder takes the vector representation of the input sentence and produces an output sentence from that representation. In the case of an RNN decoder, it does it in steps, decoding the output one token at a time using the current state and what has been decoded so far. Okay, now we have a high-level understanding of the encoder-decoder architecture, how do we train it?

=== The Training Phase

To train a model, you need a dataset that is a collection of input/output pairs that you want your model to imitate, you can then feed that dataset to the model, which will correct its own weights during training on the basis of the error it produces on a given input in the dataset. This error is essentially the difference between what the neural networks generates given an input sequence and the true output sequence you have in the dataset.

How do you produce this data set? In the case of the encoder-decoder architecture this is a bit more complicated than for typical predictive models. First you need a collection of input and output texts. In the case of translation, that would be sentence pairs where one sentence is in the source language while the other is the translation. You need feed the source language sentence to the encoder and then compute the error between what the decoder generates and the actual translation. However, there is a catch. The decoder also needs its own input at training time. You need to give the decoder the correct previous translated token as input to generate the next token rather than what the decoder has generated so far. This methods of training is called teacher forcing because you force the decoder to generate the next token from the correct previous token. This means that in your code you have to prepare two input sentence, the original one fed to the encoder and also the original one shifted to the left that you’ll feef to the decoder.

Another subtle point is that the decoder generates at each step only the probability that each token in your vocabulary is the next one. Using these probabilities you have to select a word, and there are several approaches for that. This simplest one called Grid Search is to generate the token that has the highest probability. A better approach that produces better results is called Beam Search, in that case, you use the probabilities generated by decoder to evaluate the probability of sentence chunks, rather than individual words. And you keep at each step the most likely generated chunk. That’s how training is done. Now let’s move on the serving phase.

=== The Serving Phase

After training at serving time, when you want to, say generate a new translation or a new response to a prompt, you’ll start by feeding the encoder representation of the prompt to the decoder along with a special token like go, this will prompt the decoder to generate the first word. Let’s see in more details what happens during the generation stage. First of all, the start token needs to be represented by a vector using an embedding layer, to be represented by a vector using an embedding layer. Then the recurrent layer will update the previous state produced by the encoder into a new state. This state will be passed to a dense softmax layer to produce the word probabilities. Finally the word is generating by taking the highest probability word with Greddy Search or the highest probability chunk with Beam Search. At this point you repeat this procedure for the second word to be generated, and for the third one, until you’re done.

== Attention Mechanism


