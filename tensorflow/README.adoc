= TensorFlow
:toc: manual

https://www.tensorflow.org/

== N-dimensional

[cols="2,2,5a,5a"]
|===
|Common Name |Rank(Dimension) |Example |Shape

|Scalar
|0
|
----
tf.constant(3)
----
|
----
()
----

|Vector
|1
|
----
tf.constant([3, 5, 7])
----
|
----
(3,)
----

|Matix
|2
|
----
tf.constant([[3, 5, 7], [4, 6, 8]])
----
|
----
(2, 3)
----

|3D Tensor
|3
|
----
tf.constant([[[3, 5, 7], [4, 6, 8]],
             [[1, 2, 3], [4, 5, 5]]])
----
|
----
(2, 2, 3)
----

|nD Tensor
|n
|
----
x1 = tf.constant([3, 5, 7])
x2 = tf.stack([x1, x1])
x3 = tf.stack([x2, x2, x2, x2])
x4 = tf.stack([x3, x3])
----
|
----
(2, 4, 2, 3)
----

|===

[source, python]
.*Tensor slice*
----
import tensorflow as tf

m = tf.constant([[3, 5, 7], [4, 6, 8]])
x = m[:, 2]
y = m[1, :]

print(m)
print(x)
print(y)
----

[source, python]
----
tf.Tensor(
[[3 5 7]
 [4 6 8]], shape=(2, 3), dtype=int32)
tf.Tensor([7 8], shape=(2,), dtype=int32)
tf.Tensor([4 6 8], shape=(3,), dtype=int32)
----

[source, python]
.*Tensor reshape*
----
import tensorflow as tf

m = tf.constant([[3, 5, 7], [4, 6, 8]])
x = tf.reshape(m, [3,2])
y = tf.reshape(m, [1,6])
z = tf.reshape(m, [6,1])

print(m)
print(x)
print(y)
print(z)
----

[source, python]
----
tf.Tensor(
[[3 5 7]
 [4 6 8]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[3 5]
 [7 4]
 [6 8]], shape=(3, 2), dtype=int32)
tf.Tensor([[3 5 7 4 6 8]], shape=(1, 6), dtype=int32)
tf.Tensor(
[[3]
 [5]
 [7]
 [4]
 [6]
 [8]], shape=(6, 1), dtype=int32)
----

[source, python]
.*Tensor variables(addition, subtraction, multiplication, division)*
----
import tensorflow as tf

x = tf.Variable(2.0, dtype=tf.float32, name="x")
y = tf.Variable(3.0, dtype=tf.float32, name="y")
z = tf.Variable(2.0, dtype=tf.float32, name="z")
print(x)
x.assign(45.8)
print(x)
x.assign_add(4)
print(x)
x.assign_sub(3)
print(x)
x.assign(tf.multiply(x, y))
print(x)
x.assign(tf.divide(x, z))
print(x)
----

[source, python]
----
<tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>
<tf.Variable 'x:0' shape=() dtype=float32, numpy=45.8>
<tf.Variable 'x:0' shape=() dtype=float32, numpy=49.8>
<tf.Variable 'x:0' shape=() dtype=float32, numpy=46.8>
<tf.Variable 'x:0' shape=() dtype=float32, numpy=140.4>
<tf.Variable 'x:0' shape=() dtype=float32, numpy=70.2>
----

== Quickstart

[source, python]
.*1. Set up TensorFlow*
----
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
----

[source, python]
.*2. Load a dataset*
----
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
----

[source, python]
.*3. Build a machine learning model*
----
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

predictions = model(x_train[:1]).numpy()
print(predictions)
----

NOTE: Sequential is useful for stacking layers where each layer has one input tensor and one output tensor. Layers are functions with a known mathematical structure that can be reused and have trainable variables. Most TensorFlow models are composed of layers. 

NOTE: *tf.keras.layers.Flatte* 用于将输入数据展平成一维向量。

NOTE: *tf.keras.layers.Dense* 用于创建全连接（密集连接）的神经网络层。它的作用是将输入数据与权重矩阵相乘，并加上偏置，然后将结果传递给激活函数。

NOTE: *tf.keras.layers.Dropout* 用于在训练过程中应用 dropout 正则化。Dropout 是一种常用的正则化技术，通过在训练期间随机丢弃网络中的一部分节点（按照指定的丢弃率），来减少过拟合的风险。

[source, python]
.*softmax function*
----
tf.nn.softmax(predictions).numpy()
----

[source, python]
.*loss function*
----
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss_fn(y_train[:1], predictions).numpy()
----

[source, python]
.*compile the model*
----
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])
----

[source, python]
.*4. Train and evaluate your model*
----
model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test,  y_test, verbose=2)
probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])

probability_model(x_test[:5])
----

[source, python]
.**
----

----

[source, python]
.**
----

----
